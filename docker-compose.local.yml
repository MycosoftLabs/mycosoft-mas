version: "3.9"

# =============================================================================
# MYCOSOFT MAS - Local Development Docker Compose
# =============================================================================
# Usage:
#   make up              # Start core services
#   make up-local        # Start with local LLM (Ollama)
#   make up-observability # Start with full monitoring stack
#
# Profiles:
#   - (default): Core MAS services + dependencies
#   - local-llm: Adds Ollama for local model inference
#   - observability: Adds Loki, Promtail for log aggregation
# =============================================================================

services:
  # ===========================================================================
  # CORE MAS SERVICES
  # ===========================================================================
  
  mas-orchestrator:
    build:
      context: .
      dockerfile: Dockerfile
      target: runtime
    container_name: mas-orchestrator
    ports:
      - "${MAS_PORT:-8001}:8000"
    command: >
      sh -c "wait-for-it postgres:5432 -t 60 -- 
             wait-for-it redis:6379 -t 60 -- 
             wait-for-it qdrant:6333 -t 60 --
             python -m uvicorn mycosoft_mas.core.myca_main:app --host 0.0.0.0 --port 8000"
    environment:
      - MAS_ENV=${MAS_ENV:-development}
      - DEBUG_MODE=${DEBUG_MODE:-true}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FORMAT=${LOG_FORMAT:-json}
      # Database
      - DATABASE_URL=postgresql://${POSTGRES_USER:-mas}:${POSTGRES_PASSWORD:-maspassword}@postgres:5432/${POSTGRES_DB:-mas}
      # Redis
      - REDIS_URL=redis://redis:6379/0
      # Qdrant
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      # LLM Configuration
      - LLM_DEFAULT_PROVIDER=${LLM_DEFAULT_PROVIDER:-openai}
      - LLM_BASE_URL=${LLM_BASE_URL:-http://litellm:4000}
      - LLM_API_KEY=${LLM_API_KEY:-}
      - LLM_MODEL_PLANNING=${LLM_MODEL_PLANNING:-gpt-4o}
      - LLM_MODEL_EXECUTION=${LLM_MODEL_EXECUTION:-gpt-4o-mini}
      - LLM_MODEL_FAST=${LLM_MODEL_FAST:-gpt-4o-mini}
      - LLM_MODEL_EMBEDDING=${LLM_MODEL_EMBEDDING:-text-embedding-3-small}
      # Security
      - SECRET_KEY=${SECRET_KEY:-dev-secret-key-change-in-production}
      - APPROVAL_REQUIRED=${APPROVAL_REQUIRED:-false}
      # Paths
      - PYTHONPATH=/app
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data
      - ./config:/app/config:ro
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - mas-network
    restart: unless-stopped

  # ===========================================================================
  # LLM PROXY (LiteLLM)
  # ===========================================================================
  
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: mas-litellm
    ports:
      - "${LITELLM_PORT:-4000}:4000"
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-mas-local-dev}
      - LITELLM_LOG_LEVEL=${LOG_LEVEL:-INFO}
      # Provider API Keys (loaded from env)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - AZURE_API_KEY=${AZURE_API_KEY:-}
      - AZURE_API_BASE=${AZURE_API_BASE:-}
      - AZURE_API_VERSION=${AZURE_API_VERSION:-2024-02-15-preview}
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
    volumes:
      - ./config/litellm_config.yaml:/app/config.yaml:ro
    command: ["--config", "/app/config.yaml", "--port", "4000"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - mas-network
    restart: unless-stopped

  # ===========================================================================
  # INFRASTRUCTURE SERVICES
  # ===========================================================================
  
  postgres:
    image: postgres:16-alpine
    container_name: mas-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-mas}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-maspassword}
      POSTGRES_DB: ${POSTGRES_DB:-mas}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/postgres/init.sql:/docker-entrypoint-initdb.d/01-init.sql:ro
      - ./docker/postgres/audit.sql:/docker-entrypoint-initdb.d/02-audit.sql:ro
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h localhost -U ${POSTGRES_USER:-mas}"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - mas-network
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: mas-redis
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - mas-network
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:v1.7.3
    container_name: mas-qdrant
    ports:
      - "${QDRANT_PORT:-6333}:6333"
      - "${QDRANT_GRPC_PORT:-6334}:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/readyz"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - mas-network
    restart: unless-stopped

  # ===========================================================================
  # MONITORING
  # ===========================================================================
  
  prometheus:
    image: prom/prometheus:v2.52.0
    container_name: mas-prometheus
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/rules:/etc/prometheus/rules:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=7d'
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mas-network
    restart: unless-stopped

  grafana:
    image: grafana/grafana:10.3.3
    container_name: mas-grafana
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=${GRAFANA_ROOT_URL:-http://localhost:3000}
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - mas-network
    restart: unless-stopped

  # ===========================================================================
  # OPTIONAL: LOCAL LLM (Ollama)
  # ===========================================================================
  
  ollama:
    image: ollama/ollama:latest
    container_name: mas-ollama
    profiles:
      - local-llm
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - mas-network
    restart: unless-stopped

  # ===========================================================================
  # OPTIONAL: OBSERVABILITY (Log aggregation)
  # ===========================================================================
  
  loki:
    image: grafana/loki:2.9.0
    container_name: mas-loki
    profiles:
      - observability
    ports:
      - "${LOKI_PORT:-3100}:3100"
    volumes:
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - mas-network
    restart: unless-stopped

  promtail:
    image: grafana/promtail:2.9.0
    container_name: mas-promtail
    profiles:
      - observability
    volumes:
      - ./promtail-config.yaml:/etc/promtail/config.yml:ro
      - ./logs:/var/log/mas:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      - loki
    networks:
      - mas-network
    restart: unless-stopped

# =============================================================================
# VOLUMES
# =============================================================================

volumes:
  postgres_data:
    name: mas-postgres-data
  redis_data:
    name: mas-redis-data
  qdrant_data:
    name: mas-qdrant-data
  prometheus_data:
    name: mas-prometheus-data
  grafana_data:
    name: mas-grafana-data
  ollama_data:
    name: mas-ollama-data
  loki_data:
    name: mas-loki-data

# =============================================================================
# NETWORKS
# =============================================================================

networks:
  mas-network:
    name: mas-network
    driver: bridge

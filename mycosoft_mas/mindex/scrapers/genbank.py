"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }






"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }







"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }






"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }








"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }






"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }







"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }






"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }








"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }






"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }







"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }






"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }











"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }






"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }







"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }






"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }








"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }






"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }







"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }






"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }








"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }






"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }







"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }






"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }





"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }


"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }

"""
GenBank/NCBI Scraper for MINDEX

Fetches fungal genomic data from GenBank/NCBI.
API Documentation: https://www.ncbi.nlm.nih.gov/books/NBK25500/

Focus areas:
- Fungal genome sequences
- Taxonomic information
- Gene annotations
- ITS sequences for identification
"""

import logging
from datetime import datetime
from typing import Any, AsyncIterator, Optional
from xml.etree import ElementTree

from .base import BaseScraper, ScraperConfig, ScraperResult

logger = logging.getLogger(__name__)

# NCBI Taxonomy ID for Fungi
FUNGI_TAXON_ID = 4751


class GenBankScraper(BaseScraper):
    """Scraper for NCBI GenBank fungal genomic data."""
    
    def __init__(self, config: Optional[ScraperConfig] = None, api_key: Optional[str] = None):
        # NCBI allows 3 requests/second with API key, 1/second without
        if config is None:
            rate = 3.0 if api_key else 0.33
            config = ScraperConfig(rate_limit_per_second=rate)
        super().__init__(config)
        self.api_key = api_key
    
    @property
    def source_name(self) -> str:
        return "GenBank"
    
    @property
    def base_url(self) -> str:
        return "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    
    def _get_default_headers(self) -> dict[str, str]:
        headers = super()._get_default_headers()
        headers["Accept"] = "application/json,application/xml"
        return headers
    
    def _build_params(self, params: dict) -> dict:
        """Add API key to params if available."""
        if self.api_key:
            params["api_key"] = self.api_key
        return params
    
    async def search_species(
        self,
        query: str,
        limit: int = 100,
    ) -> ScraperResult:
        """Search for fungal species in taxonomy database."""
        records = []
        errors = []
        
        try:
            # Search taxonomy database
            search_params = self._build_params({
                "db": "taxonomy",
                "term": f'"{query}"[Scientific Name] AND "Fungi"[Division]',
                "retmax": min(limit, 100),
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch details for found IDs
                    fetch_params = self._build_params({
                        "db": "taxonomy",
                        "id": ",".join(ids),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for tax_id in ids:
                            if tax_id in details["result"]:
                                normalized = self.normalize_record(details["result"][tax_id])
                                if self.validate_record(normalized):
                                    records.append(normalized)
                        
        except Exception as e:
            logger.error(f"Error searching GenBank taxonomy: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="species_search",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"query": query},
        )
    
    async def get_species_details(
        self,
        species_id: str,
    ) -> Optional[dict[str, Any]]:
        """Get detailed taxonomy information."""
        try:
            params = self._build_params({
                "db": "taxonomy",
                "id": species_id,
                "retmode": "json",
            })
            
            data = await self._request("esummary.fcgi", params=params)
            
            if data and "result" in data and species_id in data["result"]:
                return self.normalize_record(data["result"][species_id])
        except Exception as e:
            logger.error(f"Error getting species {species_id}: {e}")
        return None
    
    async def get_nucleotide_sequences(
        self,
        taxon_id: int,
        gene_type: str = "ITS",
        limit: int = 100,
    ) -> ScraperResult:
        """Get nucleotide sequences for a taxon (e.g., ITS for fungi ID)."""
        records = []
        errors = []
        
        try:
            # Search for sequences
            search_params = self._build_params({
                "db": "nucleotide",
                "term": f"txid{taxon_id}[Organism] AND {gene_type}[Title]",
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    # Fetch sequence summaries
                    fetch_params = self._build_params({
                        "db": "nucleotide",
                        "id": ",".join(ids[:50]),  # Limit batch size
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for seq_id in ids[:50]:
                            if seq_id in details["result"]:
                                records.append(self._normalize_sequence(details["result"][seq_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching sequences: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="sequences",
            records=records,
            total_count=len(records),
            errors=errors,
            metadata={"taxon_id": taxon_id, "gene_type": gene_type},
        )
    
    async def fetch_fungal_genomes(
        self,
        limit: int = 100,
    ) -> ScraperResult:
        """Fetch fungal genome assemblies."""
        records = []
        errors = []
        
        try:
            # Search genome database for fungi
            search_params = self._build_params({
                "db": "genome",
                "term": '"Fungi"[Organism]',
                "retmax": limit,
                "retmode": "json",
            })
            
            data = await self._request("esearch.fcgi", params=search_params)
            
            if data and "esearchresult" in data:
                ids = data["esearchresult"].get("idlist", [])
                
                if ids:
                    fetch_params = self._build_params({
                        "db": "genome",
                        "id": ",".join(ids[:50]),
                        "retmode": "json",
                    })
                    
                    details = await self._request("esummary.fcgi", params=fetch_params)
                    
                    if details and "result" in details:
                        for genome_id in ids[:50]:
                            if genome_id in details["result"]:
                                records.append(self._normalize_genome(details["result"][genome_id]))
                        
        except Exception as e:
            logger.error(f"Error fetching genomes: {e}")
            errors.append(str(e))
        
        return ScraperResult(
            source=self.source_name,
            data_type="genomes",
            records=records,
            total_count=len(records),
            errors=errors,
        )
    
    async def fetch_all(
        self,
        limit: Optional[int] = None,
    ) -> AsyncIterator[ScraperResult]:
        """Fetch all fungal taxonomy data in batches."""
        retstart = 0
        max_records = limit or self.config.max_records or 10000
        total_fetched = 0
        
        while total_fetched < max_records:
            batch_size = min(self.config.batch_size, max_records - total_fetched)
            
            try:
                # Search taxonomy for all fungi
                search_params = self._build_params({
                    "db": "taxonomy",
                    "term": '"Fungi"[Division]',
                    "retmax": batch_size,
                    "retstart": retstart,
                    "retmode": "json",
                })
                
                data = await self._request("esearch.fcgi", params=search_params)
                
                if not data or "esearchresult" not in data:
                    break
                
                ids = data["esearchresult"].get("idlist", [])
                
                if not ids:
                    break
                
                # Fetch details
                fetch_params = self._build_params({
                    "db": "taxonomy",
                    "id": ",".join(ids),
                    "retmode": "json",
                })
                
                details = await self._request("esummary.fcgi", params=fetch_params)
                
                records = []
                if details and "result" in details:
                    for tax_id in ids:
                        if tax_id in details["result"]:
                            normalized = self.normalize_record(details["result"][tax_id])
                            if self.validate_record(normalized):
                                records.append(normalized)
                
                if records:
                    yield ScraperResult(
                        source=self.source_name,
                        data_type="taxonomy",
                        records=records,
                        total_count=int(data["esearchresult"].get("count", 0)),
                    )
                
                total_fetched += len(records)
                retstart += batch_size
                
                logger.info(f"Fetched {total_fetched} taxonomy records from GenBank")
                
            except Exception as e:
                logger.error(f"Error in fetch_all: {e}")
                yield ScraperResult(
                    source=self.source_name,
                    data_type="taxonomy",
                    records=[],
                    total_count=0,
                    errors=[str(e)],
                )
                break
    
    def validate_record(self, record: dict[str, Any]) -> bool:
        """Validate a GenBank record."""
        if not record:
            return False
        return bool(record.get("scientific_name") or record.get("source_id"))
    
    def normalize_record(self, taxon: dict[str, Any]) -> dict[str, Any]:
        """Normalize a taxonomy record to MINDEX format."""
        return {
            "source": self.source_name,
            "source_id": str(taxon.get("taxid", taxon.get("uid", ""))),
            "scientific_name": taxon.get("scientificname", ""),
            "common_name": taxon.get("commonname", ""),
            "rank": taxon.get("rank", ""),
            "taxonomy": {
                "kingdom": "Fungi",
                "division": taxon.get("division", ""),
                "lineage": taxon.get("lineage", ""),
            },
            "genbank_id": taxon.get("uid", ""),
            "scraped_at": datetime.utcnow().isoformat(),
            "raw_data": taxon,
        }
    
    def _normalize_sequence(self, seq: dict[str, Any]) -> dict[str, Any]:
        """Normalize a sequence record."""
        return {
            "source": self.source_name,
            "source_id": str(seq.get("uid", "")),
            "type": "sequence",
            "accession": seq.get("accessionversion", ""),
            "title": seq.get("title", ""),
            "organism": seq.get("organism", ""),
            "sequence_length": seq.get("slen", 0),
            "molecule_type": seq.get("moltype", ""),
            "update_date": seq.get("updatedate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }
    
    def _normalize_genome(self, genome: dict[str, Any]) -> dict[str, Any]:
        """Normalize a genome assembly record."""
        return {
            "source": self.source_name,
            "source_id": str(genome.get("uid", "")),
            "type": "genome",
            "organism_name": genome.get("organism_name", ""),
            "assembly_accession": genome.get("assembly_accession", ""),
            "assembly_name": genome.get("assembly_name", ""),
            "genome_size": genome.get("total_length", 0),
            "scaffold_count": genome.get("scaffold_count", 0),
            "contig_count": genome.get("contig_count", 0),
            "submitter": genome.get("submitter", ""),
            "submission_date": genome.get("submissiondate", ""),
            "scraped_at": datetime.utcnow().isoformat(),
        }












